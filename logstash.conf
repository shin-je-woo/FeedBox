input {
    kafka {
        bootstrap_servers => "kafka1:19092,kafka2:19092,kafka3:19092"
        topics => ["feedbox.accesslog"]
        group_id => "feedbox-logstash-access-log"
        consumer_threads => 3
        auto_offset_reset => "earliest"
        codec => "json"
        type => "kafka"
    }
}

# 비정형 데이터를 정형 데이터로 변환하기 위해 정규표현식처럼 파싱한다.
filter {
    grok {
        match => { "message" => "%{IPORHOST:client_ip} - - \[%{HTTPDATE:access_at}\] \"%{WORD:http_method} %{URIPATH:request_path} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_status} %{NUMBER:bytes} %{NUMBER:duration} \"%{DATA:referrer}\" \"%{DATA:user_agent}\"" }
    }
    if "_grokparsefailure" in [tags] {
        drop {}
    }
    date {
        match => [ "access_at", "dd/MMM/yyyy:HH:mm:ss Z" ]
        target => "access_at"
    }
    mutate {
        remove_field => [ "message" ]
    }
}

output {
    elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "access-log-%{+YYYY-MM-dd}"
    }
}